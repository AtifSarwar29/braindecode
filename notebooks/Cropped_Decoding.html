

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Cropped Decoding &mdash; Braindecode 0.4.85 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Braindecode 0.4.85 documentation" href="../index.html"/>
        <link rel="next" title="Trialwise Manual Training Loop" href="Trialwise_Manual_Training_Loop.html"/>
        <link rel="prev" title="Trialwise Decoding" href="Trialwise_Decoding.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Braindecode
          

          
          </a>

          
            
            
              <div class="version">
                0.4.85
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Trialwise_Decoding.html">Trialwise Decoding</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cropped Decoding</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Enable-logging">Enable logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Load-data">Load data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Convert-data-to-Braindecode-format">Convert data to Braindecode format</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Create-the-model">Create the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Run-the-training">Run the training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Dataset-references">Dataset references</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Trialwise_Manual_Training_Loop.html">Trialwise Manual Training Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="Cropped_Manual_Training_Loop.html">Cropped Manual Training Loop</a></li>
<li class="toctree-l1"><a class="reference internal" href="visualization/Perturbation.html">Amplitude Perturbation Visualization</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.datautil.html">braindecode.datautil package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.experiments.html">braindecode.experiments package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.mne_ext.html">braindecode.mne_ext package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.models.html">braindecode.models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.torch_ext.html">braindecode.torch_ext package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../source/braindecode.visualization.html">braindecode.visualization package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Braindecode</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Cropped Decoding</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/Cropped_Decoding.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 5ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Cropped-Decoding">
<h1>Cropped Decoding<a class="headerlink" href="#Cropped-Decoding" title="Permalink to this headline">¶</a></h1>
<p>Now we will use cropped decoding. Cropped decoding means the ConvNet is trained on time windows/time crops within the trials. We will explain this visually by comparing trialwise to cropped decoding.</p>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="47%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Trialwise Decoding</th>
<th class="head">Cropped Decoding</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><img alt="Trialwise Decoding" src="../_images/trialwise_explanation.png" /></td>
<td><img alt="Cropped Decoding" src="../_images/cropped_explanation.png" /></td>
</tr>
</tbody>
</table>
<p>On the left, you see trialwise decoding:</p>
<ol class="arabic simple">
<li>A complete trial is pushed through the network</li>
<li>The network produces a prediction</li>
<li>The prediction is compared to the target (label) for that trial to compute the loss</li>
</ol>
<p>On the right, you see cropped decoding:</p>
<ol class="arabic simple">
<li>Instead of a complete trial, windows within the trial, here called <em>crops</em>, are pushed through the network</li>
<li>For computational efficiency, multiple neighbouring crops are pushed through the network simultaneously (these neighbouring crops are called a <em>supercrop</em>)</li>
<li>Therefore, the network produces multiple predictions (one per crop in the supercrop)</li>
<li>The individual crop predictions are averaged before computing the loss function</li>
</ol>
<p>Notes:</p>
<ul class="simple">
<li>The network architecture implicitly defines the crop size (it is the receptive field size, i.e., the number of timesteps the network uses to make a single prediction)</li>
<li>The supercrop size is a user-defined hyperparameter, called <code class="docutils literal"><span class="pre">input_time_length</span></code> in Braindecode. It mostly affects runtime (larger supercrop sizes should be faster). As a rule of thumb, you can set it to two times the crop size.</li>
<li>Crop size and supercrop size together define how many predictions the network makes per supercrop: <span class="math">\(\mathrm{\#supercrop}-\mathrm{\#crop}+1=\mathrm{\#predictions}\)</span></li>
</ul>
<p>For cropped decoding, the above training setup is mathematically identical to sampling crops in your dataset, pushing them through the network and training directly on the individual crops. At the same time, the above training setup is much faster as it avoids redundant computations by using dilated convolutions, see our paper <a class="reference external" href="https://arxiv.org/abs/1703.05051">Deep learning with convolutional neural networks for EEG decoding and visualization</a>. However, the two setups are only mathematically
identical in case (1) your network does not use any padding and (2) your loss function leads to the same gradients when using the averaged output. The first is true for our shallow and deep ConvNet models and the second is true for the log-softmax outputs and negative log likelihood loss that is typically used for classification in PyTorch.</p>
<p>Most of the code for cropped decoding is identical to the <a class="reference external" href="Trialwise_Decoding.html">Trialwise Decoding Tutorial</a>, differences are explained in the text.</p>
<div class="section" id="Enable-logging">
<h2>Enable logging<a class="headerlink" href="#Enable-logging" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import logging
import importlib
importlib.reload(logging) # see https://stackoverflow.com/a/21475297/1469195
log = logging.getLogger()
log.setLevel(&#39;INFO&#39;)
import sys
logging.basicConfig(format=&#39;%(asctime)s %(levelname)s : %(message)s&#39;,
                     level=logging.INFO, stream=sys.stdout)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-data">
<h2>Load data<a class="headerlink" href="#Load-data" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import mne
from mne.io import concatenate_raws

# 5,6,7,10,13,14 are codes for executed and imagined hands/feet
subject_id = 22
event_codes = [5,6,9,10,13,14]
#event_codes = [3,4,5,6,7,8,9,10,11,12,13,14]

# This will download the files if you don&#39;t have them yet,
# and then return the paths to the files.
physionet_paths = mne.datasets.eegbci.load_data(subject_id, event_codes)

# Load each of the files
parts = [mne.io.read_raw_edf(path, preload=True,stim_channel=&#39;auto&#39;, verbose=&#39;WARNING&#39;)
         for path in physionet_paths]

# Concatenate them
raw = concatenate_raws(parts)

# Find the events in this dataset
events, _ = mne.events_from_annotations(raw)

# Use only EEG channels
eeg_channel_inds = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False,
                   exclude=&#39;bads&#39;)

# Extract trials, only using EEG channels
epoched = mne.Epochs(raw, events, dict(hands_or_left=2, feet_or_right=3), tmin=1, tmax=4.1, proj=False, picks=eeg_channel_inds,
                baseline=None, preload=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Convert-data-to-Braindecode-format">
<h2>Convert data to Braindecode format<a class="headerlink" href="#Convert-data-to-Braindecode-format" title="Permalink to this headline">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import numpy as np
# Convert data from volt to millivolt
# Pytorch expects float32 for input and int64 for labels.
X = (epoched.get_data() * 1e6).astype(np.float32)
y = (epoched.events[:,2] - 2).astype(np.int64) #2,3 -&gt; 0,1
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from braindecode.datautil.signal_target import SignalAndTarget

train_set = SignalAndTarget(X[:40], y=y[:40])
valid_set = SignalAndTarget(X[40:70], y=y[40:70])

</pre></div>
</div>
</div>
</div>
<div class="section" id="Create-the-model">
<h2>Create the model<a class="headerlink" href="#Create-the-model" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
As in the trialwise decoding tutorial, we will use the Braindecode model class directly to perform the training in a few lines of code. If you instead want to use your own training loop, have a look at the <a class="reference external" href="./Cropped_Manual_Training_Loop.html">Cropped Manual Training Loop Tutorial</a>.</div>
<p>For cropped decoding, we now transform the model into a model that outputs a dense time series of predictions. For this, we manually set the length of the final convolution layer to some length that makes the receptive field of the ConvNet smaller than the number of samples in a trial (see <code class="docutils literal"><span class="pre">final_conv_length=12</span></code> in the model definition).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from braindecode.models.shallow_fbcsp import ShallowFBCSPNet
from torch import nn
from braindecode.torch_ext.util import set_random_seeds

# Set if you want to use GPU
# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.
cuda = False
set_random_seeds(seed=20170629, cuda=cuda)
n_classes = 2
in_chans = train_set.X.shape[1]

model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes,
                        input_time_length=None,
                        final_conv_length=12)
if cuda:
    model.cuda()

</pre></div>
</div>
</div>
<p>Now we supply <code class="docutils literal"><span class="pre">cropped=True</span></code> to our compile function</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>from braindecode.torch_ext.optimizers import AdamW
import torch.nn.functional as F
#optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model
optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)
model.compile(loss=F.nll_loss, optimizer=optimizer,  iterator_seed=1, cropped=True)
</pre></div>
</div>
</div>
</div>
<div class="section" id="Run-the-training">
<h2>Run the training<a class="headerlink" href="#Run-the-training" title="Permalink to this headline">¶</a></h2>
<p>For fitting, we must supply the super crop size. Here, we it to 450 by setting <code class="docutils literal"><span class="pre">input_time_length</span> <span class="pre">=</span> <span class="pre">450</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>input_time_length = 450
model.fit(train_set.X, train_set.y, epochs=30, batch_size=64, scheduler=&#39;cosine&#39;,
          input_time_length=input_time_length,
         validation_data=(valid_set.X, valid_set.y),)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2019-05-27 13:48:25,992 INFO : Run until first stop...
2019-05-27 13:48:28,722 INFO : Epoch 0
2019-05-27 13:48:28,736 INFO : train_loss                16.71894
2019-05-27 13:48:28,737 INFO : valid_loss                16.03879
2019-05-27 13:48:28,738 INFO : train_misclass            0.52500
2019-05-27 13:48:28,738 INFO : valid_misclass            0.53333
2019-05-27 13:48:28,739 INFO : runtime                   0.00000
2019-05-27 13:48:28,740 INFO :
2019-05-27 13:48:32,953 INFO : Time only for training updates: 4.21s
2019-05-27 13:48:35,792 INFO : Epoch 1
2019-05-27 13:48:35,806 INFO : train_loss                5.75666
2019-05-27 13:48:35,807 INFO : valid_loss                5.05206
2019-05-27 13:48:35,808 INFO : train_misclass            0.52500
2019-05-27 13:48:35,809 INFO : valid_misclass            0.53333
2019-05-27 13:48:35,810 INFO : runtime                   6.95663
2019-05-27 13:48:35,810 INFO :
2019-05-27 13:48:39,861 INFO : Time only for training updates: 4.05s
2019-05-27 13:48:42,728 INFO : Epoch 2
2019-05-27 13:48:42,740 INFO : train_loss                3.64768
2019-05-27 13:48:42,741 INFO : valid_loss                3.01621
2019-05-27 13:48:42,742 INFO : train_misclass            0.50000
2019-05-27 13:48:42,743 INFO : valid_misclass            0.53333
2019-05-27 13:48:42,743 INFO : runtime                   6.90803
2019-05-27 13:48:42,745 INFO :
2019-05-27 13:48:47,101 INFO : Time only for training updates: 4.35s
2019-05-27 13:48:50,364 INFO : Epoch 3
2019-05-27 13:48:50,375 INFO : train_loss                2.34256
2019-05-27 13:48:50,376 INFO : valid_loss                1.90961
2019-05-27 13:48:50,377 INFO : train_misclass            0.50000
2019-05-27 13:48:50,377 INFO : valid_misclass            0.53333
2019-05-27 13:48:50,378 INFO : runtime                   7.24010
2019-05-27 13:48:50,379 INFO :
2019-05-27 13:48:54,500 INFO : Time only for training updates: 4.12s
2019-05-27 13:48:57,328 INFO : Epoch 4
2019-05-27 13:48:57,329 INFO : train_loss                1.50676
2019-05-27 13:48:57,330 INFO : valid_loss                1.29046
2019-05-27 13:48:57,330 INFO : train_misclass            0.47500
2019-05-27 13:48:57,331 INFO : valid_misclass            0.50000
2019-05-27 13:48:57,331 INFO : runtime                   7.39941
2019-05-27 13:48:57,332 INFO :
2019-05-27 13:49:01,566 INFO : Time only for training updates: 4.23s
2019-05-27 13:49:04,563 INFO : Epoch 5
2019-05-27 13:49:04,564 INFO : train_loss                1.01924
2019-05-27 13:49:04,565 INFO : valid_loss                0.96845
2019-05-27 13:49:04,565 INFO : train_misclass            0.40000
2019-05-27 13:49:04,566 INFO : valid_misclass            0.43333
2019-05-27 13:49:04,567 INFO : runtime                   7.06577
2019-05-27 13:49:04,567 INFO :
2019-05-27 13:49:08,780 INFO : Time only for training updates: 4.21s
2019-05-27 13:49:11,724 INFO : Epoch 6
2019-05-27 13:49:11,725 INFO : train_loss                0.75856
2019-05-27 13:49:11,725 INFO : valid_loss                0.82128
2019-05-27 13:49:11,726 INFO : train_misclass            0.35000
2019-05-27 13:49:11,727 INFO : valid_misclass            0.43333
2019-05-27 13:49:11,727 INFO : runtime                   7.21428
2019-05-27 13:49:11,728 INFO :
2019-05-27 13:49:14,435 INFO : Time only for training updates: 2.70s
2019-05-27 13:49:15,841 INFO : Epoch 7
2019-05-27 13:49:15,842 INFO : train_loss                0.61156
2019-05-27 13:49:15,843 INFO : valid_loss                0.75854
2019-05-27 13:49:15,844 INFO : train_misclass            0.30000
2019-05-27 13:49:15,844 INFO : valid_misclass            0.43333
2019-05-27 13:49:15,845 INFO : runtime                   5.65511
2019-05-27 13:49:15,845 INFO :
2019-05-27 13:49:17,842 INFO : Time only for training updates: 1.99s
2019-05-27 13:49:19,281 INFO : Epoch 8
2019-05-27 13:49:19,282 INFO : train_loss                0.51892
2019-05-27 13:49:19,283 INFO : valid_loss                0.74045
2019-05-27 13:49:19,283 INFO : train_misclass            0.25000
2019-05-27 13:49:19,284 INFO : valid_misclass            0.36667
2019-05-27 13:49:19,285 INFO : runtime                   3.40579
2019-05-27 13:49:19,285 INFO :
2019-05-27 13:49:21,225 INFO : Time only for training updates: 1.94s
2019-05-27 13:49:22,572 INFO : Epoch 9
2019-05-27 13:49:22,573 INFO : train_loss                0.43383
2019-05-27 13:49:22,574 INFO : valid_loss                0.71042
2019-05-27 13:49:22,574 INFO : train_misclass            0.20000
2019-05-27 13:49:22,575 INFO : valid_misclass            0.30000
2019-05-27 13:49:22,576 INFO : runtime                   3.38393
2019-05-27 13:49:22,576 INFO :
2019-05-27 13:49:24,529 INFO : Time only for training updates: 1.95s
2019-05-27 13:49:25,871 INFO : Epoch 10
2019-05-27 13:49:25,872 INFO : train_loss                0.34278
2019-05-27 13:49:25,873 INFO : valid_loss                0.65221
2019-05-27 13:49:25,873 INFO : train_misclass            0.17500
2019-05-27 13:49:25,874 INFO : valid_misclass            0.26667
2019-05-27 13:49:25,874 INFO : runtime                   3.30348
2019-05-27 13:49:25,875 INFO :
2019-05-27 13:49:27,789 INFO : Time only for training updates: 1.91s
2019-05-27 13:49:29,232 INFO : Epoch 11
2019-05-27 13:49:29,233 INFO : train_loss                0.27082
2019-05-27 13:49:29,234 INFO : valid_loss                0.60061
2019-05-27 13:49:29,235 INFO : train_misclass            0.15000
2019-05-27 13:49:29,236 INFO : valid_misclass            0.26667
2019-05-27 13:49:29,236 INFO : runtime                   3.26077
2019-05-27 13:49:29,237 INFO :
2019-05-27 13:49:31,249 INFO : Time only for training updates: 2.01s
2019-05-27 13:49:32,650 INFO : Epoch 12
2019-05-27 13:49:32,651 INFO : train_loss                0.21861
2019-05-27 13:49:32,652 INFO : valid_loss                0.56316
2019-05-27 13:49:32,652 INFO : train_misclass            0.02500
2019-05-27 13:49:32,653 INFO : valid_misclass            0.26667
2019-05-27 13:49:32,654 INFO : runtime                   3.45901
2019-05-27 13:49:32,654 INFO :
2019-05-27 13:49:34,595 INFO : Time only for training updates: 1.94s
2019-05-27 13:49:35,937 INFO : Epoch 13
2019-05-27 13:49:35,938 INFO : train_loss                0.18199
2019-05-27 13:49:35,939 INFO : valid_loss                0.53569
2019-05-27 13:49:35,939 INFO : train_misclass            0.00000
2019-05-27 13:49:35,940 INFO : valid_misclass            0.23333
2019-05-27 13:49:35,940 INFO : runtime                   3.34639
2019-05-27 13:49:35,941 INFO :
2019-05-27 13:49:37,856 INFO : Time only for training updates: 1.91s
2019-05-27 13:49:39,194 INFO : Epoch 14
2019-05-27 13:49:39,195 INFO : train_loss                0.15484
2019-05-27 13:49:39,196 INFO : valid_loss                0.50841
2019-05-27 13:49:39,196 INFO : train_misclass            0.00000
2019-05-27 13:49:39,197 INFO : valid_misclass            0.20000
2019-05-27 13:49:39,197 INFO : runtime                   3.26092
2019-05-27 13:49:39,198 INFO :
2019-05-27 13:49:41,124 INFO : Time only for training updates: 1.92s
2019-05-27 13:49:42,468 INFO : Epoch 15
2019-05-27 13:49:42,470 INFO : train_loss                0.13389
2019-05-27 13:49:42,470 INFO : valid_loss                0.48080
2019-05-27 13:49:42,471 INFO : train_misclass            0.00000
2019-05-27 13:49:42,471 INFO : valid_misclass            0.16667
2019-05-27 13:49:42,472 INFO : runtime                   3.26746
2019-05-27 13:49:42,473 INFO :
2019-05-27 13:49:44,442 INFO : Time only for training updates: 1.96s
2019-05-27 13:49:45,779 INFO : Epoch 16
2019-05-27 13:49:45,780 INFO : train_loss                0.11739
2019-05-27 13:49:45,781 INFO : valid_loss                0.45401
2019-05-27 13:49:45,781 INFO : train_misclass            0.00000
2019-05-27 13:49:45,782 INFO : valid_misclass            0.13333
2019-05-27 13:49:45,783 INFO : runtime                   3.31831
2019-05-27 13:49:45,783 INFO :
2019-05-27 13:49:47,699 INFO : Time only for training updates: 1.91s
2019-05-27 13:49:49,038 INFO : Epoch 17
2019-05-27 13:49:49,040 INFO : train_loss                0.10472
2019-05-27 13:49:49,040 INFO : valid_loss                0.43017
2019-05-27 13:49:49,041 INFO : train_misclass            0.00000
2019-05-27 13:49:49,042 INFO : valid_misclass            0.16667
2019-05-27 13:49:49,042 INFO : runtime                   3.25675
2019-05-27 13:49:49,043 INFO :
2019-05-27 13:49:50,972 INFO : Time only for training updates: 1.92s
2019-05-27 13:49:52,316 INFO : Epoch 18
2019-05-27 13:49:52,317 INFO : train_loss                0.09406
2019-05-27 13:49:52,318 INFO : valid_loss                0.40693
2019-05-27 13:49:52,318 INFO : train_misclass            0.00000
2019-05-27 13:49:52,319 INFO : valid_misclass            0.16667
2019-05-27 13:49:52,320 INFO : runtime                   3.27348
2019-05-27 13:49:52,320 INFO :
2019-05-27 13:49:54,240 INFO : Time only for training updates: 1.91s
2019-05-27 13:49:55,578 INFO : Epoch 19
2019-05-27 13:49:55,579 INFO : train_loss                0.08521
2019-05-27 13:49:55,579 INFO : valid_loss                0.38600
2019-05-27 13:49:55,580 INFO : train_misclass            0.00000
2019-05-27 13:49:55,581 INFO : valid_misclass            0.13333
2019-05-27 13:49:55,581 INFO : runtime                   3.26806
2019-05-27 13:49:55,582 INFO :
2019-05-27 13:49:57,498 INFO : Time only for training updates: 1.91s
2019-05-27 13:49:59,176 INFO : Epoch 20
2019-05-27 13:49:59,177 INFO : train_loss                0.07795
2019-05-27 13:49:59,178 INFO : valid_loss                0.36742
2019-05-27 13:49:59,179 INFO : train_misclass            0.00000
2019-05-27 13:49:59,179 INFO : valid_misclass            0.13333
2019-05-27 13:49:59,180 INFO : runtime                   3.25760
2019-05-27 13:49:59,182 INFO :
2019-05-27 13:50:01,469 INFO : Time only for training updates: 2.28s
2019-05-27 13:50:02,840 INFO : Epoch 21
2019-05-27 13:50:02,841 INFO : train_loss                0.07171
2019-05-27 13:50:02,842 INFO : valid_loss                0.35085
2019-05-27 13:50:02,843 INFO : train_misclass            0.00000
2019-05-27 13:50:02,843 INFO : valid_misclass            0.10000
2019-05-27 13:50:02,844 INFO : runtime                   3.97155
2019-05-27 13:50:02,844 INFO :
2019-05-27 13:50:05,975 INFO : Time only for training updates: 3.13s
2019-05-27 13:50:08,928 INFO : Epoch 22
2019-05-27 13:50:08,931 INFO : train_loss                0.06681
2019-05-27 13:50:08,933 INFO : valid_loss                0.33726
2019-05-27 13:50:08,934 INFO : train_misclass            0.00000
2019-05-27 13:50:08,936 INFO : valid_misclass            0.10000
2019-05-27 13:50:08,940 INFO : runtime                   4.50735
2019-05-27 13:50:08,942 INFO :
2019-05-27 13:50:12,972 INFO : Time only for training updates: 4.02s
2019-05-27 13:50:15,773 INFO : Epoch 23
2019-05-27 13:50:15,775 INFO : train_loss                0.06289
2019-05-27 13:50:15,777 INFO : valid_loss                0.32600
2019-05-27 13:50:15,779 INFO : train_misclass            0.00000
2019-05-27 13:50:15,780 INFO : valid_misclass            0.10000
2019-05-27 13:50:15,782 INFO : runtime                   6.99638
2019-05-27 13:50:15,784 INFO :
2019-05-27 13:50:19,843 INFO : Time only for training updates: 4.05s
2019-05-27 13:50:22,641 INFO : Epoch 24
2019-05-27 13:50:22,644 INFO : train_loss                0.05999
2019-05-27 13:50:22,646 INFO : valid_loss                0.31735
2019-05-27 13:50:22,648 INFO : train_misclass            0.00000
2019-05-27 13:50:22,649 INFO : valid_misclass            0.10000
2019-05-27 13:50:22,651 INFO : runtime                   6.87178
2019-05-27 13:50:22,653 INFO :
2019-05-27 13:50:26,612 INFO : Time only for training updates: 3.95s
2019-05-27 13:50:29,313 INFO : Epoch 25
2019-05-27 13:50:29,316 INFO : train_loss                0.05785
2019-05-27 13:50:29,318 INFO : valid_loss                0.31063
2019-05-27 13:50:29,334 INFO : train_misclass            0.00000
2019-05-27 13:50:29,335 INFO : valid_misclass            0.10000
2019-05-27 13:50:29,337 INFO : runtime                   6.76969
2019-05-27 13:50:29,339 INFO :
2019-05-27 13:50:33,272 INFO : Time only for training updates: 3.93s
2019-05-27 13:50:36,087 INFO : Epoch 26
2019-05-27 13:50:36,090 INFO : train_loss                0.05635
2019-05-27 13:50:36,091 INFO : valid_loss                0.30562
2019-05-27 13:50:36,093 INFO : train_misclass            0.00000
2019-05-27 13:50:36,095 INFO : valid_misclass            0.10000
2019-05-27 13:50:36,096 INFO : runtime                   6.65909
2019-05-27 13:50:36,098 INFO :
2019-05-27 13:50:40,122 INFO : Time only for training updates: 4.02s
2019-05-27 13:50:42,913 INFO : Epoch 27
2019-05-27 13:50:42,916 INFO : train_loss                0.05532
2019-05-27 13:50:42,919 INFO : valid_loss                0.30180
2019-05-27 13:50:42,921 INFO : train_misclass            0.00000
2019-05-27 13:50:42,922 INFO : valid_misclass            0.10000
2019-05-27 13:50:42,924 INFO : runtime                   6.84971
2019-05-27 13:50:42,926 INFO :
2019-05-27 13:50:46,889 INFO : Time only for training updates: 3.96s
2019-05-27 13:50:49,706 INFO : Epoch 28
2019-05-27 13:50:49,709 INFO : train_loss                0.05461
2019-05-27 13:50:49,711 INFO : valid_loss                0.29879
2019-05-27 13:50:49,713 INFO : train_misclass            0.00000
2019-05-27 13:50:49,714 INFO : valid_misclass            0.10000
2019-05-27 13:50:49,716 INFO : runtime                   6.76746
2019-05-27 13:50:49,718 INFO :
2019-05-27 13:50:53,661 INFO : Time only for training updates: 3.94s
2019-05-27 13:50:56,538 INFO : Epoch 29
2019-05-27 13:50:56,541 INFO : train_loss                0.05415
2019-05-27 13:50:56,543 INFO : valid_loss                0.29639
2019-05-27 13:50:56,544 INFO : train_misclass            0.00000
2019-05-27 13:50:56,546 INFO : valid_misclass            0.10000
2019-05-27 13:50:56,548 INFO : runtime                   6.77185
2019-05-27 13:50:56,550 INFO :
2019-05-27 13:51:00,538 INFO : Time only for training updates: 3.98s
2019-05-27 13:51:03,489 INFO : Epoch 30
2019-05-27 13:51:03,492 INFO : train_loss                0.05384
2019-05-27 13:51:03,494 INFO : valid_loss                0.29441
2019-05-27 13:51:03,496 INFO : train_misclass            0.00000
2019-05-27 13:51:03,497 INFO : valid_misclass            0.10000
2019-05-27 13:51:03,499 INFO : runtime                   6.87712
2019-05-27 13:51:03,501 INFO :
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>&lt;braindecode.experiments.experiment.Experiment at 0x7fea66414e80&gt;
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>model.epochs_df
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>train_misclass</th>
      <th>valid_misclass</th>
      <th>runtime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16.718943</td>
      <td>16.038786</td>
      <td>0.525</td>
      <td>0.533333</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.756664</td>
      <td>5.052062</td>
      <td>0.525</td>
      <td>0.533333</td>
      <td>6.956634</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.647677</td>
      <td>3.016214</td>
      <td>0.500</td>
      <td>0.533333</td>
      <td>6.908031</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.342561</td>
      <td>1.909614</td>
      <td>0.500</td>
      <td>0.533333</td>
      <td>7.240101</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.506759</td>
      <td>1.290458</td>
      <td>0.475</td>
      <td>0.500000</td>
      <td>7.399405</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1.019237</td>
      <td>0.968446</td>
      <td>0.400</td>
      <td>0.433333</td>
      <td>7.065770</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.758563</td>
      <td>0.821283</td>
      <td>0.350</td>
      <td>0.433333</td>
      <td>7.214281</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.611557</td>
      <td>0.758545</td>
      <td>0.300</td>
      <td>0.433333</td>
      <td>5.655106</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.518917</td>
      <td>0.740452</td>
      <td>0.250</td>
      <td>0.366667</td>
      <td>3.405795</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.433832</td>
      <td>0.710416</td>
      <td>0.200</td>
      <td>0.300000</td>
      <td>3.383932</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.342782</td>
      <td>0.652211</td>
      <td>0.175</td>
      <td>0.266667</td>
      <td>3.303478</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.270821</td>
      <td>0.600610</td>
      <td>0.150</td>
      <td>0.266667</td>
      <td>3.260772</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.218609</td>
      <td>0.563165</td>
      <td>0.025</td>
      <td>0.266667</td>
      <td>3.459012</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.181991</td>
      <td>0.535691</td>
      <td>0.000</td>
      <td>0.233333</td>
      <td>3.346393</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.154839</td>
      <td>0.508405</td>
      <td>0.000</td>
      <td>0.200000</td>
      <td>3.260915</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.133887</td>
      <td>0.480797</td>
      <td>0.000</td>
      <td>0.166667</td>
      <td>3.267457</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.117386</td>
      <td>0.454011</td>
      <td>0.000</td>
      <td>0.133333</td>
      <td>3.318306</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.104724</td>
      <td>0.430165</td>
      <td>0.000</td>
      <td>0.166667</td>
      <td>3.256755</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.094064</td>
      <td>0.406926</td>
      <td>0.000</td>
      <td>0.166667</td>
      <td>3.273476</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.085207</td>
      <td>0.386002</td>
      <td>0.000</td>
      <td>0.133333</td>
      <td>3.268065</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.077945</td>
      <td>0.367422</td>
      <td>0.000</td>
      <td>0.133333</td>
      <td>3.257596</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.071709</td>
      <td>0.350847</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>3.971547</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.066811</td>
      <td>0.337260</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>4.507347</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.062891</td>
      <td>0.325995</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.996382</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.059993</td>
      <td>0.317353</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.871776</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.057849</td>
      <td>0.310633</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.769685</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.056350</td>
      <td>0.305617</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.659091</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.055315</td>
      <td>0.301796</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.849715</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.054613</td>
      <td>0.298791</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.767460</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.054149</td>
      <td>0.296389</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.771852</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.053844</td>
      <td>0.294413</td>
      <td>0.000</td>
      <td>0.100000</td>
      <td>6.877121</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Eventually, we arrive at 90% accuracy, so 27 from 30 trials are correctly predicted.</p>
</div>
<div class="section" id="Evaluation">
<h2>Evaluation<a class="headerlink" href="#Evaluation" title="Permalink to this headline">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>test_set = SignalAndTarget(X[70:], y=y[70:])

model.evaluate(test_set.X, test_set.y)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>{&#39;loss&#39;: 0.4238094389438629,
 &#39;misclass&#39;: 0.09999999999999998,
 &#39;runtime&#39;: 0.00041961669921875}
</pre></div>
</div>
</div>
<p>We can now predict entire trials as before or individual crops.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>model.predict_classes(test_set.X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>model.predict_outs(test_set.X)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>array([[-1.2750863e+00, -4.7210640e-01],
       [-6.4165510e-02, -3.2850025e+00],
       [-1.2179451e+00, -9.6833640e-01],
       [-9.6923895e-02, -2.7711761e+00],
       [-1.5608791e-02, -4.4860415e+00],
       [-3.0281614e-03, -6.5692320e+00],
       [-3.3592165e+00, -9.9474899e-02],
       [-5.2215487e-01, -1.3793353e+00],
       [-7.2297215e-02, -3.2223554e+00],
       [-1.8260284e-01, -2.6842451e+00],
       [-1.1441530e+00, -5.1276696e-01],
       [-2.6686563e+00, -2.3944007e-01],
       [-3.9805791e-01, -1.4446534e+00],
       [-8.5448694e-01, -5.9342915e-01],
       [-4.0538985e-01, -1.5777966e+00],
       [-1.4246053e+00, -4.7016972e-01],
       [-1.7791660e+00, -3.6660773e-01],
       [-1.0101254e+00, -6.9896615e-01],
       [-2.7277711e-01, -2.1119595e+00],
       [-1.4588629e+00, -4.5223159e-01]], dtype=float32)
</pre></div>
</div>
</div>
<p>For individual crops, provide <code class="docutils literal"><span class="pre">individual_crops=True</span></code>. This for example can be useful to plot accuracies over time:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib
%matplotlib inline
matplotlib.style.use(&#39;seaborn&#39;)
labels_per_trial_per_crop = model.predict_classes(test_set.X, individual_crops=True)
accs_per_crop = [l == y for l,y in zip(labels_per_trial_per_crop, test_set.y)]
accs_per_crop = np.mean(accs_per_crop, axis=0)
plt.figure(figsize=(8,3))
plt.plot(accs_per_crop * 100)
plt.title(&quot;Accuracies per timestep&quot;, fontsize=16)
plt.xlabel(&#39;Timestep in trial&#39;, fontsize=14)
plt.ylabel(&#39;Accuracy [%]&#39;, fontsize=14)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>Text(0, 0.5, &#39;Accuracy [%]&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Cropped_Decoding_33_1.png" src="../_images/notebooks_Cropped_Decoding_33_1.png" />
</div>
</div>
<p>Raw outputs could be used to visualize a prediction probability across a trial. Here we look at the first trial.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span>cropped_outs = model.predict_outs(test_set.X, individual_crops=True)
i_trial = 0
# log-softmax outputs need to be exponentiated to get probabilities
cropped_probs = np.exp(cropped_outs[i_trial])
plt.figure(figsize=(8,3))
plt.plot(cropped_probs.T)
plt.title(&quot;Network probabilities for trial {:d} of class {:d}&quot;.format(
    i_trial, test_set.y[i_trial]), fontsize=16)
plt.legend((&quot;Class 0&quot;, &quot;Class 1&quot;), fontsize=12)
plt.xlabel(&quot;Timestep within trial&quot;, fontsize=14)
plt.ylabel(&quot;Probabilities&quot;, fontsize=14)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area highlight-none"><div class="highlight"><pre>
<span></span>Text(0, 0.5, &#39;Probabilities&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_Cropped_Decoding_35_1.png" src="../_images/notebooks_Cropped_Decoding_35_1.png" />
</div>
</div>
</div>
<div class="section" id="Dataset-references">
<h2>Dataset references<a class="headerlink" href="#Dataset-references" title="Permalink to this headline">¶</a></h2>
<p>This dataset was created and contributed to PhysioNet by the developers of the <a class="reference external" href="http://www.schalklab.org/research/bci2000">BCI2000</a> instrumentation system, which they used in making these recordings. The system is described in:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Schalk</span><span class="p">,</span> <span class="n">G</span><span class="o">.</span><span class="p">,</span> <span class="n">McFarland</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">J</span><span class="o">.</span><span class="p">,</span> <span class="n">Hinterberger</span><span class="p">,</span> <span class="n">T</span><span class="o">.</span><span class="p">,</span> <span class="n">Birbaumer</span><span class="p">,</span> <span class="n">N</span><span class="o">.</span><span class="p">,</span> <span class="n">Wolpaw</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">R</span><span class="o">.</span> <span class="p">(</span><span class="mi">2004</span><span class="p">)</span> <span class="n">BCI2000</span><span class="p">:</span> <span class="n">A</span> <span class="n">General</span><span class="o">-</span><span class="n">Purpose</span> <span class="n">Brain</span><span class="o">-</span><span class="n">Computer</span> <span class="n">Interface</span> <span class="p">(</span><span class="n">BCI</span><span class="p">)</span> <span class="n">System</span><span class="o">.</span> <span class="n">IEEE</span> <span class="n">TBME</span> <span class="mi">51</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span><span class="mi">1034</span><span class="o">-</span><span class="mf">1043.</span>
</pre></div>
</div>
<p><a class="reference external" href="https://physionet.org/physiobank/">PhysioBank</a> is a large and growing archive of well-characterized digital recordings of physiologic signals and related data for use by the biomedical research community and further described in:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Goldberger</span> <span class="n">AL</span><span class="p">,</span> <span class="n">Amaral</span> <span class="n">LAN</span><span class="p">,</span> <span class="n">Glass</span> <span class="n">L</span><span class="p">,</span> <span class="n">Hausdorff</span> <span class="n">JM</span><span class="p">,</span> <span class="n">Ivanov</span> <span class="n">PCh</span><span class="p">,</span> <span class="n">Mark</span> <span class="n">RG</span><span class="p">,</span> <span class="n">Mietus</span> <span class="n">JE</span><span class="p">,</span> <span class="n">Moody</span> <span class="n">GB</span><span class="p">,</span> <span class="n">Peng</span> <span class="n">C</span><span class="o">-</span><span class="n">K</span><span class="p">,</span> <span class="n">Stanley</span> <span class="n">HE</span><span class="o">.</span> <span class="p">(</span><span class="mi">2000</span><span class="p">)</span> <span class="n">PhysioBank</span><span class="p">,</span> <span class="n">PhysioToolkit</span><span class="p">,</span> <span class="ow">and</span> <span class="n">PhysioNet</span><span class="p">:</span> <span class="n">Components</span> <span class="n">of</span> <span class="n">a</span> <span class="n">New</span> <span class="n">Research</span> <span class="n">Resource</span> <span class="k">for</span> <span class="n">Complex</span> <span class="n">Physiologic</span> <span class="n">Signals</span><span class="o">.</span> <span class="n">Circulation</span> <span class="mi">101</span><span class="p">(</span><span class="mi">23</span><span class="p">):</span><span class="n">e215</span><span class="o">-</span><span class="n">e220</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Trialwise_Manual_Training_Loop.html" class="btn btn-neutral float-right" title="Trialwise Manual Training Loop" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Trialwise_Decoding.html" class="btn btn-neutral" title="Trialwise Decoding" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Robin Tibor Schirrmeister.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.4.85',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>